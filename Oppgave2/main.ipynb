{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading in the data\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4465, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Taking a look at the data\n",
    "\n",
    "# Helper function to display the image\n",
    "def imshow(img):\n",
    "    # Un-normalize and display the image\n",
    "    img = img / 2 + 0.5  # assuming normalization was applied earlier\n",
    "    # Convert from tensor image (C x H x W) to (H x W x C)\n",
    "    plt.imshow(np.transpose(img, (1, 2, 0)))\n",
    "\n",
    "# Get one batch of training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)  # Fetch the next batch of images and labels\n",
    "\n",
    "# Convert images to numpy for display (if they are in torch.Tensor format)\n",
    "images = images.numpy()\n",
    "\n",
    "# Plot the images in the batch\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "\n",
    "# Display 20 images\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 10, idx + 1, xticks=[], yticks=[])\n",
    "    imshow(images[idx])\n",
    "    ax.set_title(classes[labels[idx]])\n",
    "\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training and Test data\n",
    "\n",
    "total_samples = 40000\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "# Total desired dataset size: 20,000 samples (15,000 training, 5,000 validation)\n",
    "num_total_samples = 30000\n",
    "num_train_samples = 20000\n",
    "num_val_samples = 10000\n",
    "\n",
    "# Get the indices of the \"frog\" class (class 6)\n",
    "frog_indices = [i for i, label in enumerate(trainset.targets) if label == 6]\n",
    "\n",
    "# Get the indices of other classes\n",
    "other_indices = [i for i, label in enumerate(trainset.targets) if label != 6]\n",
    "\n",
    "# Ensure 50% of the training data consists of \"frog\" images (7,500 frog images)\n",
    "num_frog_train = num_train_samples // 2\n",
    "num_frog_val = num_val_samples // 2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select \"frog\" images for training and validation\n",
    "\n",
    "train_frog_indices = frog_indices[:num_frog_train]\n",
    "val_frog_indices = frog_indices[num_frog_train:num_frog_train + num_frog_val]\n",
    "\n",
    "# Randomly select the rest of the images (not frog) for the remaining training and validation data\n",
    "remaining_train_samples = num_train_samples - num_frog_train\n",
    "remaining_val_samples = num_val_samples - num_frog_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fill the rest of the training data with the other categories\n",
    "\n",
    "# Randomly shuffle the other class indices\n",
    "np.random.shuffle(other_indices)\n",
    "\n",
    "# Select the remaining training and validation indices\n",
    "train_other_indices = other_indices[:remaining_train_samples]\n",
    "val_other_indices = other_indices[remaining_train_samples:remaining_train_samples + remaining_val_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combine the \"frog\" and \"other\" indices for training and validation sets\n",
    "\n",
    "train_indices = train_frog_indices + train_other_indices\n",
    "val_indices = val_frog_indices + val_other_indices\n",
    "\n",
    "# Create the subsets for training and validation\n",
    "train_subset = Subset(trainset, train_indices)\n",
    "val_subset = Subset(trainset, val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 15000\n",
      "Validation set size: 5000\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoader for training and validation subsets\n",
    "batch_size = 64\n",
    "trainloader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "valloader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "# Check the sizes of the datasets\n",
    "print(f'Training set size: {len(trainloader.dataset)}')\n",
    "print(f'Validation set size: {len(valloader.dataset)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Donwsampling av datasettet for å få modellen til å lære kategorien bedre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv_layer1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv_layer2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (dropout1): Dropout2d(p=0.25, inplace=False)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=400, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class FrogClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FrogClassifier, self).__init__()\n",
    "        # First Convolutional Layer (Input: 3 channels, Output: 6, Kernel: 5x5)\n",
    "        self.conv_layer1 = nn.Conv2d(3, 6, 5)  \n",
    "        # Second Convolutional Layer (Input: 6 channels, Output: 16, Kernel: 5x5)\n",
    "        self.conv_layer2 = nn.Conv2d(6, 16, 5)  \n",
    "        \n",
    "        # Dropout layers\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 128)  # Adjusted input size\n",
    "        self.fc2 = nn.Linear(128, 10)  # Output layer (10 classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        # Convolutional Layer 1 + ReLU + Pooling\n",
    "        data = F.relu(self.conv_layer1(data))\n",
    "        data = F.max_pool2d(data, 2)  \n",
    "\n",
    "        # Convolutional Layer 2 + ReLU + Pooling\n",
    "        data = F.relu(self.conv_layer2(data))\n",
    "        data = F.max_pool2d(data, 2)  \n",
    "\n",
    "        # Apply dropout\n",
    "        data = self.dropout1(data)\n",
    "\n",
    "        # Flatten tensor for fully connected layers\n",
    "        data = torch.flatten(data, 1)\n",
    "\n",
    "        # Fully Connected Layer 1 + ReLU + Dropout\n",
    "        data = F.relu(self.fc1(data))\n",
    "        data = self.dropout2(data)\n",
    "\n",
    "        # Output Layer\n",
    "        data = self.fc2(data)\n",
    "\n",
    "        return F.log_softmax(data, dim=1)\n",
    "\n",
    "# Instantiate the model\n",
    "model = FrogClassifier()\n",
    "\n",
    "# Function to initialize weights\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)  # Move model to GPU\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining a loss function\n",
    "\n",
    "# Loss function used to measure how well the models prediction match the actual labels\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Updates the models loss during traning with SDG optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/235], Loss: 2.2938\n",
      "Epoch [1/10], Step [200/235], Loss: 1.9004\n",
      "Epoch [2/10], Step [100/235], Loss: 1.7683\n",
      "Epoch [2/10], Step [200/235], Loss: 1.6838\n",
      "Epoch [3/10], Step [100/235], Loss: 1.6106\n",
      "Epoch [3/10], Step [200/235], Loss: 1.5848\n",
      "Epoch [4/10], Step [100/235], Loss: 1.5145\n",
      "Epoch [4/10], Step [200/235], Loss: 1.5118\n",
      "Epoch [5/10], Step [100/235], Loss: 1.4505\n",
      "Epoch [5/10], Step [200/235], Loss: 1.4513\n",
      "Epoch [6/10], Step [100/235], Loss: 1.4175\n",
      "Epoch [6/10], Step [200/235], Loss: 1.4001\n",
      "Epoch [7/10], Step [100/235], Loss: 1.3571\n",
      "Epoch [7/10], Step [200/235], Loss: 1.3868\n",
      "Epoch [8/10], Step [100/235], Loss: 1.3332\n",
      "Epoch [8/10], Step [200/235], Loss: 1.3320\n",
      "Epoch [9/10], Step [100/235], Loss: 1.3209\n",
      "Epoch [9/10], Step [200/235], Loss: 1.3344\n",
      "Epoch [10/10], Step [100/235], Loss: 1.3095\n",
      "Epoch [10/10], Step [200/235], Loss: 1.2751\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Train the network\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)  # Move the model to GPU if available\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):  # epochs\n",
    "    \n",
    "    # Total loss over all batches\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        \n",
    "        # Extracting inputs and labels from the batch\n",
    "        inputs, labels = data \n",
    "        \n",
    "        # Move inputs and labels to GPU (if available)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass: Get model predictions\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backpropagation: Calculate gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Perform gradient descent to update the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Add the loss to the running total\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Print the loss every 100 steps\n",
    "        if i % 100 == 99:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(trainloader)}], Loss: {running_loss / 100:.4f}')\n",
    "            running_loss = 0.0 \n",
    "\n",
    "print(\"Finished Training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
